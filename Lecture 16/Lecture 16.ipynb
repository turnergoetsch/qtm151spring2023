{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\"> Lecture 16 - Practicing Chaining </span>\n",
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "In this lecture you will get a chance to practice <br>\n",
    "the main dataset operations\n",
    "\n",
    "- There will be a quiz on this lecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\"> I. Import Libraries and Data </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = pd.read_csv(\"data_raw/results.csv\")\n",
    "races    = pd.read_csv(\"data_raw/races.csv\")\n",
    "results[\"points col\"] = results[\"points\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\"> II. Review Dataset Operations </span>\n",
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "See attached file for a refresher on syntax\n",
    "\n",
    "```[] ``` $\\qquad \\qquad \\qquad \\quad$: Extracting columns <br>\n",
    "```.query() ``` $\\qquad \\qquad $: Subsetting rows <br>\n",
    "```.recode() ``` $ \\qquad \\quad \\ \\ $: Replacing values <br>\n",
    "```.groupby().agg() ```: Aggregate statistics by subgroup <br>\n",
    "```.rename() ``` $\\qquad \\quad \\ \\ $: Change name of columns\n",
    "\n",
    "Full list:\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "https://www.w3schools.com/python/pandas/pandas_ref_dataframe.asp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\"> III. Examples of Chaining </span>\n",
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "The operations with \".\" are read left to right\n",
    "\n",
    "- Combine any of the above operations\n",
    "- Great way to make code efficient\n",
    "- The sky's the limit!\n",
    "\n",
    "\n",
    "Subsetting **before** extracting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driverId</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20320</th>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20344</th>\n",
       "      <td>18</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20368</th>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20392</th>\n",
       "      <td>18</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20416</th>\n",
       "      <td>17</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25740</th>\n",
       "      <td>830</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25760</th>\n",
       "      <td>830</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25780</th>\n",
       "      <td>830</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25800</th>\n",
       "      <td>847</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25820</th>\n",
       "      <td>830</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       driverId  points\n",
       "20320         4    25.0\n",
       "20344        18    25.0\n",
       "20368        20    25.0\n",
       "20392        18    25.0\n",
       "20416        17    25.0\n",
       "...         ...     ...\n",
       "25740       830    25.0\n",
       "25760       830    25.0\n",
       "25780       830    25.0\n",
       "25800       847    26.0\n",
       "25820       830    25.0\n",
       "\n",
       "[262 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data for drivers that scored more than 20 points on individual races\n",
    "# Then extract the columns \"driverId\" and \"points\"\n",
    "results.query('points >= 20')[[\"driverId\",\"points\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "Subsetting **before** aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_points</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constructorId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.904924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.203911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.910966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.012821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.809028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>3.723684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2.327869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>3.693182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean_points\n",
       "constructorId             \n",
       "1                 3.148148\n",
       "3                 1.904924\n",
       "4                 1.903226\n",
       "5                 1.203911\n",
       "6                 4.910966\n",
       "...                    ...\n",
       "209               0.012821\n",
       "210               0.809028\n",
       "211               3.723684\n",
       "213               2.327869\n",
       "214               3.693182\n",
       "\n",
       "[176 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This obtains a subset of drivers who competed in races 500 onwards\n",
    "# then computes the average by team (\"constructorId\")\n",
    "\n",
    "(results.query('raceId >= 500')\n",
    "        .groupby(\"constructorId\")\n",
    "        .agg(mean_points = (\"points\",\"mean\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "Subsetting **after** aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_points</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constructorId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>12.363643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean_points\n",
       "constructorId             \n",
       "131              12.363643"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This obtains the average points by team (\"constructorId\"), then \n",
    "# produces a subset of team whose average is higher than 10\n",
    "\n",
    "(results.groupby(\"constructorId\")\n",
    "        .agg(mean_points = (\"points\",\"mean\"))\n",
    "        .query('mean_points >= 10'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "Chaining inside queries + NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"is.na()\" produces a True/False vector, checking for missing values\n",
    "# \"is.notna()\" produces a True/False vector, checking for non-missing values\n",
    "# .str.isnumeric() is used for checking whether individual rows of a\n",
    "# string column are numeric.\n",
    "results[\"points\"].isna()\n",
    "results[\"points\"].notna()\n",
    "\n",
    "subset_nas    = results.query('points.isna()')\n",
    "subset_nonnas = results.query('points.notna()')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkblue\"> III. Quiz Structure </span>\n",
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "The day of the quiz I will ...\n",
    "- Provide a dataset with information\n",
    "- Give more specific instructions.\n",
    "- Below, you will see the type of questions that will be asked.\n",
    "- The idea is for you to apply known concepts to new data\n",
    "- You have 50 minutes to complete the assignment\n",
    "\n",
    "Questions\n",
    "\n",
    "(exact wording may change in quiz, but exercise will be very similar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "(a) Replace the values of a column\n",
    "\n",
    "- Obtain unique string values of a column\n",
    "- Use the \".replace()\" command\n",
    "\n",
    "Hint: See Lecture 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'file_name.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mfile_name.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m#Step 1: Obtain unique string values of the column using the unique() method. For example, if you want to obtain the unique string values of a column named \"column_name\", you can do:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m unique_values \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcolumn_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file_name.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('file_name.csv')\n",
    "\n",
    "#Step 1: Obtain unique string values of the column using the unique() method. For example, if you want to obtain the unique string values of a column named \"column_name\", you can do:\n",
    "\n",
    "unique_values = df['column_name'].unique()\n",
    "\n",
    "#Step 2: Create a dictionary that maps the old values to the new values that you want to replace them with. For example, if you want to replace \"old_value1\" with \"new_value1\" and \"old_value2\" with \"new_value2\", you can create the dictionary as follows:\n",
    "\n",
    "replace_dict = {\n",
    "    'old_value1': 'new_value1',\n",
    "    'old_value2': 'new_value2'\n",
    "}\n",
    "\n",
    "#Step 3: Finally, use the .replace() method to replace the old values with the new values in the column. You can use a for loop to iterate over each unique value in the column and replace it with the corresponding new value. For example:\n",
    "for value in unique_values:\n",
    "    df['column_name'] = df['column_name'].replace(value, replace_dict.get(value))\n",
    "\n",
    "#This will replace all the old values in the \"column_name\" column with the new values specified in the replace_dict dictionary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "(b) Recode a numeric column\n",
    "\n",
    "- Use the \"pd.cut()\" command to create <br>\n",
    "a new column based on an interval.\n",
    "- See Lecture 14 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Step 1: Determine the intervals or bins for your numeric column. For example, if you have a numeric column named \"column_name\" and you want to bin it into three intervals, you can use the pd.cut() command as follows:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m bins \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mcut(df[\u001b[39m'\u001b[39m\u001b[39mcolumn_name\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Step 2: The pd.cut() command creates three intervals of equal width based on the range of the values in the \"column_name\" column. Create a new column in the DataFrame to store the binned values. For example, if you want to store the binned values in a new column named \"binned_column\", you can do:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mbinned_column\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m bins\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Determine the intervals or bins for your numeric column. For example, if you have a numeric column named \"column_name\" and you want to bin it into three intervals, you can use the pd.cut() command as follows:\n",
    "\n",
    "bins = pd.cut(df['column_name'], 3)\n",
    "\n",
    "# Step 2: The pd.cut() command creates three intervals of equal width based on the range of the values in the \"column_name\" column. Create a new column in the DataFrame to store the binned values. For example, if you want to store the binned values in a new column named \"binned_column\", you can do:\n",
    "\n",
    "df['binned_column'] = bins\n",
    "\n",
    "# Step 3: This will add a new column to the DataFrame named \"binned_column\" containing the binned values. Finally, you can optionally rename the binned values using the labels parameter in pd.cut(). For example, if you want to rename the binned values as \"low\", \"medium\", and \"high\", you can do:\n",
    "\n",
    "bins = pd.cut(df['column_name'], 3, labels=['low', 'medium', 'high'])\n",
    "df['binned_column'] = bins\n",
    "\n",
    "# Overall: This will bin the values in the \"column_name\" column into three intervals with labels \"low\", \"medium\", and \"high\" and store the binned values in the new column named \"binned_column\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "(c) Aggregate and query\n",
    "\n",
    "- Use a combniation of the following commands <br>\n",
    "to produce a new dataset <br>\n",
    "``` .query() ``` <br>\n",
    "``` .groupby().agg() ``` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Step 1: Use the .query() command to filter the dataset based on certain conditions. For example, if you want to select only the rows where the value of a variable named \"variable_name\" is greater than 10, you can do:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#This will filter the dataset to include only the rows where the value of \"variable_name\" is greater than 10.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39mvariable_name > 10\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m#Step 2: Use the .groupby().agg() command to aggregate the variables in the dataset. For example, if you want to calculate the mean value of a variable named \"variable_name\" for each value of another variable named \"group_variable\", you can do:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#This will group the dataset by the \"group_variable\" variable and calculate the mean value of \"variable_name\" for each group.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m grouped_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mgroup_variable\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mvariable_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Step 1: Use the .query() command to filter the dataset based on certain conditions. For example, if you want to select only the rows where the value of a variable named \"variable_name\" is greater than 10, you can do:\n",
    "#This will filter the dataset to include only the rows where the value of \"variable_name\" is greater than 10.\n",
    "\n",
    "df = df.query('variable_name > 10')\n",
    "\n",
    "#Step 2: Use the .groupby().agg() command to aggregate the variables in the dataset. For example, if you want to calculate the mean value of a variable named \"variable_name\" for each value of another variable named \"group_variable\", you can do:\n",
    "#This will group the dataset by the \"group_variable\" variable and calculate the mean value of \"variable_name\" for each group.\n",
    "\n",
    "grouped_df = df.groupby('group_variable').agg({'variable_name': 'mean'})\n",
    "\n",
    "#Step 3: Finally, you can optionally reset the index of the aggregated dataset using the .reset_index() command. For example, if you want to reset the index of the \"grouped_df\" dataset, you can do:\n",
    "\n",
    "grouped_df = grouped_df.reset_index()\n",
    "\n",
    "#This will reset the index of the \"grouped_df\" dataset to default integer values.\n",
    "#Overall, this combination of .query() and .groupby().agg() commands can be used to produce a new dataset that is filtered and aggregated based on certain conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "(d) Aggregate and sort\n",
    "\n",
    "- Use a combniation of the following commands <br>\n",
    "to produce a new dataset <br>\n",
    "``` .groupby().agg() ``` <br>\n",
    "``` .sort_values() ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Use the .groupby().agg() command to aggregate the variables in the dataset. For example, if you want to calculate the mean value of a variable named \"variable_name\" for each value of another variable named \"group_variable\", you can do:\n",
    "\n",
    "grouped_df = df.groupby('group_variable').agg({'variable_name': 'mean'})\n",
    "\n",
    "#Step 2: This will group the dataset by the \"group_variable\" variable and calculate the mean value of \"variable_name\" for each group.Use the .sort_values() command to sort the aggregated dataset based on certain variables. For example, if you want to sort the \"grouped_df\" dataset by the mean value of \"variable_name\" in descending order, you can do:\n",
    "\n",
    "sorted_df = grouped_df.sort_values(by='variable_name', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "(e) Rename column\n",
    "\n",
    "- Create a dictionary\n",
    "- Rename one or more columns in a dataset <br>\n",
    "using the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Create a dictionary that maps the old column names to the new column names. For example, if you want to rename a column named \"old_column_name\" to \"new_column_name\", you can create a dictionary as follows:\n",
    "#This will create a dictionary that maps the old column name \"old_column_name\" to the new column name \"new_column_name\".\n",
    "\n",
    "column_name_dict = {'old_column_name': 'new_column_name'}\n",
    "\n",
    "#Step 2: Use the .rename() command to rename the columns in the dataset based on the dictionary. For example, if you want to rename the columns in the \"df\" dataset based on the \"column_name_dict\" dictionary, you can do:\n",
    "\n",
    "df = df.rename(columns=column_name_dict)\n",
    "\n",
    "#This will rename the columns in the \"df\" dataset based on the \"column_name_dict\" dictionary and store the renamed dataset in the \"df\" variable.\n",
    "#Overall, this approach allows you to rename one or more columns in a dataset using a dictionary that maps the old column names to the new column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">\n",
    "\n",
    "(f) Merge dataset\n",
    "\n",
    "- Use \"pd.merge\" to combine two datasets: <br>\n",
    "a primary and secondary\n",
    "- Only merge a subset of the columns of the <br>\n",
    "secondary dataset\n",
    "- Use \"display\" to show a the merged dataset,  <br>\n",
    "extracting a subset of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Next, read in the two datasets you want to merge. For example, if you have two datasets named \"primary_dataset.csv\" and \"secondary_dataset.csv\", you can read them in using the read_csv() method from pandas library as follows:\n",
    "\n",
    "primary_dataset = pd.read_csv('primary_dataset.csv')\n",
    "secondary_dataset = pd.read_csv('secondary_dataset.csv')\n",
    "\n",
    "#Step 2: Use the pd.merge() function to merge the two datasets. You can specify the primary dataset as the first argument, and the secondary dataset as the second argument. You can also specify the columns to merge on using the on parameter. For example, if you want to merge the datasets on a column named \"merge_column\", you can do:\n",
    "\n",
    "merged_dataset = pd.merge(primary_dataset, secondary_dataset[['merge_column', 'secondary_column']], on='merge_column')\n",
    "\n",
    "#This will display only the \"merge_column\" and \"primary_column\" columns of the merged dataset.\n",
    "#Overall, this approach allows you to merge two datasets in Python using the pd.merge() function, and select a subset of the columns to include in the merged dataset using the on and [['column_name']] parameters, and display a subset of columns using display() function.#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
